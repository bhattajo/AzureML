# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.The hyperparameters are chosen using HyperDrive. It also build a model using Azure AutoML feature on the same dataset.
This model is then compared to Scikit-learn logistic regression model.

## Summary
**This dataset contains bank marketing campaign for potential target customer.The target column is the deposit column (Y/N)**

**The solution is to run classification ML model using LogisticRegression with HyperDrive and AutoML and compare the result**

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

**HyperParameter**

HyperDrive pipeline was based on prescribed Scikit-learn Logistic Regression classifier. LogisticRegression generates a function called logit which ranges between 0 and 1. The Arewa Under the curve with max area is the best threhold which maximum accuracy. Hence suitable for binary classifiaction problems.

DataSet has 32,950 rows. Its was split at 80/20 for taining and test set. 2 fold crossvalidation is performed for better accuracy.

Hyperparameter tuned are C: regularization strength and max_iter: Maximum number of iterations. Regularization prevents model from overfitting using L1 and/or L2 Norm.

![image](https://user-images.githubusercontent.com/19474037/146861214-cf9b758c-70a3-488c-b8f6-ed82252bcacb.png)


We used HyperDrive to run multiple simulation automatically and selecting the best model. This is a trial and error method to find the best fit by running all combination in case of GridSampling or running certain parameters randomly in case of random sampling. We used RandomParameterSampling because its optimal when there is limited CPU and its faster. For more exhaustive tuning a Grid sampling may be a better choice.


**DataSet Information:**

***Bank client data :***

• 1 - age

• 2 - job : type of job

• 3 - marital : marital status

• 4 - education

• 5 - default: has credit in default?

• 6 - housing: has housing loan?

• 7 - loan: has personal loan?


***Related to previous contact :***

• 8 - contact: contact communication type

• 9 - month: last contact month of year

• 10 - day_of_week: last contact day of the week

• 11 - duration: last contact duration, in seconds


***Other attributes :***

• 12 - campaign: number of contacts performed during this campaign and for this client

• 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign

• 14 - previous: number of contacts performed before this campaign and for this client

• 15 - poutcome: outcome of the previous marketing campaign 

• 21 - has the client subscribed a term deposit (y/n)


Data was cleaned and label/one hot encoding used in clean_data method of train.py.

**What are the benefits of the parameter sampler you chose?**
Parameter sampler has 3 options , Grid, Random and Bayesian.Bayesian sampling build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. RandomSampling is faster and usullay always yield similar accuracy compared to GridSampling.

**What are the benefits of the early stopping policy you chose?**
Early stooping in general in machine learning stops the training when a certain accuracy is acieved or the accuracy gain is minimal for eachconsecutive epoch or if it runs for long time which might be an indication of Gradient Decent overshooting insteam of convering This can happen when the learning rate is large.

I used Bandit policy that defines early termination based on slack criteria and frequency and delay interval for evaluation. See below

![image](https://user-images.githubusercontent.com/19474037/146862102-6b0c1057-68bd-4dc9-9def-f263ad49f06d.png)

evaluation_interval: At what frequency to apply the policy.
slack_factor: The ratio used to calculate the allowed distance from the best performing experiment run


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

AutoML pipeline evaluated multiple classifier and was also able to do feature engineering on the raw dataset as well. These range from Logistic Regression, Random forest to ensemble classifiers like XGBoost, GradientBoost etc.See List below. It was able to tune across a large no of hyperparameter. See config settings below:

![image](https://user-images.githubusercontent.com/19474037/146862754-1e9f49ff-8295-44be-89ba-02886076c75a.png)


The model selected was VotingEnsemble.

The AutoML used AUC_Weighted as primary metrics for maximizing the Area Under the ROC curve and also performed 2 fold cross validation to minimize any bias in the training set. 
experiement timeout minute was set to 30 minutes. We didnt want to train for long because of resource constraints.

![image](https://user-images.githubusercontent.com/19474037/131574189-13310373-1f0b-4b73-824a-fcc1a4b4889a.png)

experiment_timeout_minutes=30 : How long the experiment should continue to run. To help avoid experiment running long, 30 min isused.

task='classification' : Type is classification or Regression. In this dataset its binary clasiification

primary_metric='AUC_weighted' : Area Under curve (Receiver Operating Characteristic) is used when we want to maximine the true positive and true negatives for a skewed (imbalanced) dataset.

n_cross_validations=2 : Cross validation are performed to prevent overfitting. In 2 fold cross validation the training set is divided into 2 sub-sets of training and validation set and the model is chosen whcih performs the best.



## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Logistic Regression model with 90.75% Accuracy. Best Run ID:  HD_7ee44286-bce0-4ed0-986f-98c0ef95d045_0

Azure AutoML: VotingEnsemble classifier with 91.43 % Accuracy. Id: AutoML_0f49d162-2148-4953-b79d-bf96bbe157b6_3

Hence, the best pipeline was Azure AutoML and best model was VotingEnsemble classifier with 91.43 % Accuracy.

AutoMl runs a gamut of classifiers and does incredible number crunching when it comes to HyperParameter tuning. Its possible to manually do that in the code but its would take days if not weeks. Hence we can get a verygood accuracy using AutoML.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

The dataset is imbalanced, we can use techniques like up-sampling or SMOTE to generate synthetic data points and its coule have increased the accuracy. Also, n-fold cross validation can be increased so that we distribute any noise in the dataset evenly and take model which is just about right (neither over or underfitting). Regularization like Lasso or Ridge can be used to improve the performance. 

## Proof of Cluster clean up ##

![image](https://user-images.githubusercontent.com/19474037/146865722-705c53e2-00f8-433e-ab6a-7a29f0dcce13.png)




